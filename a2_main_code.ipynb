{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# COMP-8730_Assignment-2 Spell Correction Using LM\n",
        ">## Student Information\n",
        ">* Name: Jiajie Yang\n",
        ">* UWin Acc: yang4q\n",
        ">* Student ID: 110115897"
      ],
      "metadata": {
        "id": "h-fJv_7nYGfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!pip install \"pytrec-eval-terrier\"\n",
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "id": "2KeQvXqPYt_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a9902c-dfbc-4961-9bbd-dee65079cfb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytrec-eval-terrier\n",
            "  Downloading pytrec_eval_terrier-0.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.6/286.6 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytrec-eval-terrier\n",
            "Successfully installed pytrec-eval-terrier-0.5.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OAsJ5VRh9LpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f5dc4b-01b4-4533-bdf6-9a2ea0ee30b9"
      },
      "cell_type": "code",
      "source": [
        "import pytrec_eval\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## N-gram Language Model with NLTK News Genre of Brown’s Corpus\n",
        ">> ### Prepare Data"
      ],
      "metadata": {
        "id": "YIhf-o0Q9m-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_lst = []\n",
        "for sent in brown.sents(categories=['news', 'adventure']):\n",
        "    sent_lst.append([w.lower() for w in sent])\n",
        "#print(sent_lst[1])\n",
        "#sent_lst = brown.sents()\n",
        "#brown.words(categories='news')\n",
        "#print(len([s for s in sent_lst if len(s) > 0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "fnC1og3B-CQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> ### Train Models"
      ],
      "metadata": {
        "id": "Wvc0MI4f3pZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "\n",
        "ngram_order_lst = [1,2,3,5,10]\n",
        "lm_dict = {}\n",
        "for order in ngram_order_lst:\n",
        "    train_data, vocab_data = padded_everygram_pipeline(order, sent_lst)\n",
        "    lm = MLE(order)\n",
        "    lm.fit(train_data, vocab_data)\n",
        "    lm_dict[order] = lm\n",
        "\n"
      ],
      "metadata": {
        "id": "slkkm5Ou3omN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Use Max Heap to Store Top-K Probable Tokens"
      ],
      "metadata": {
        "id": "xj7uPjkTEOhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Customized Max Heap\n",
        "class MaxHeap_Max_Score_Word:\n",
        "    \n",
        "    def __init__(self, maxsize):\n",
        "        \n",
        "        self.maxsize = maxsize\n",
        "        self.size = 0\n",
        "        self.Heap = [[]] * self.maxsize\n",
        "        self.Root = 0\n",
        "    \n",
        "    # Function to return True if self is an empty heap; False, otherwise\n",
        "    def is_empty(self):\n",
        "\n",
        "        return self.size == 0\n",
        "\n",
        "    # Function to return True if self is full; False, otherwise\n",
        "    def is_full(self):\n",
        "\n",
        "        return self.size == self.maxsize\n",
        "\n",
        "    # Function to return the index of the last index\n",
        "    def last(self):\n",
        "\n",
        "        return self.size - 1\n",
        "        \n",
        "    # Function to return the position of parent for the node currently at pos\n",
        "    def parent(self, pos):\n",
        "        \n",
        "        return (pos - 1) // 2\n",
        "\n",
        "    # Function to return the position of the left child for the node currently at pos\n",
        "    def leftChild(self, pos):\n",
        "        \n",
        "        return 2 * pos + 1\n",
        "\n",
        "    # Function to return the position of the right child for the node currently at pos\n",
        "    def rightChild(self, pos):\n",
        "        \n",
        "        return 2 * (pos + 1)\n",
        "    \n",
        "    # Function to return True if pos has left child; False, otherwise\n",
        "    def has_leftChild(self, pos):\n",
        "\n",
        "        return self.leftChild(pos) < self.size;\n",
        "\n",
        "    # Function to return True if pos has right child; False, otherwise\n",
        "    def has_rightChild(self, pos):\n",
        "        \n",
        "        return self.rightChild(pos) < self.size;\n",
        "\n",
        "    # Function that returns true if the passed node is a leaf node\n",
        "    def isLeaf(self, pos):\n",
        "        \n",
        "        return self.leftChild(pos) >= self.size and pos < self.size\n",
        "\n",
        "    # Function that returns true if the passed node is a leaf node\n",
        "    def isRoot(self, pos):\n",
        "        \n",
        "        return pos == 0\n",
        "\n",
        "    # Function to swap two nodes of the heap\n",
        "    def swap(self, pos_1, pos_2):\n",
        "        \n",
        "        tmp = self.Heap[pos_1]\n",
        "        self.Heap[pos_1] = self.Heap[pos_2]\n",
        "        self.Heap[pos_2] = tmp\n",
        "        return\n",
        "\n",
        "    # Function to heapify the node at pos\n",
        "    def maxHeapify(self, pos):\n",
        "\n",
        "        while not self.isLeaf(pos):\n",
        "            large_idx = self.leftChild(pos)\n",
        "            if self.has_rightChild(pos) and (self.Heap[self.rightChild(pos)][0] >\n",
        "                                             self.Heap[large_idx][0]):\n",
        "                large_idx = self.rightChild(pos)\n",
        "            if self.Heap[pos][0] >= self.Heap[large_idx][0]:\n",
        "                break\n",
        "            self.swap(pos, large_idx)\n",
        "            pos = large_idx\n",
        "        return\n",
        "\n",
        "    # Function to insert a node into the heap\n",
        "    def insert(self, element):\n",
        "        \n",
        "        if self.size >= self.maxsize:\n",
        "            return\n",
        "        self.size += 1\n",
        "        self.Heap[self.size - 1] = element\n",
        "\n",
        "        current = self.size -1\n",
        "\n",
        "        while (not self.isRoot(current) and \n",
        "               self.Heap[current][0] > self.Heap[self.parent(current)][0]):\n",
        "            self.swap(current, self.parent(current))\n",
        "            current = self.parent(current)\n",
        "        return\n",
        "    \n",
        "    # Function to return the maximum element from the heap\n",
        "    def top(self):\n",
        "\n",
        "        return self.Heap[self.Root]\n",
        "    \n",
        "    # Function to return the maximum element's key from the heap\n",
        "    def topKey(self):\n",
        "\n",
        "        return self.Heap[self.Root][0]\n",
        "\n",
        "    # Function to remove and return the maximum element from the heap\n",
        "    def extractMax(self):\n",
        "\n",
        "        popped = self.Heap[self.Root]\n",
        "        self.Heap[self.Root] = self.Heap[self.last()]\n",
        "        self.size -= 1\n",
        "        if not self.is_empty():\n",
        "            self.maxHeapify(self.Root)\n",
        "        \n",
        "        return popped\n",
        "\n",
        "    # Function to print the contents of the heap\n",
        "    def Print(self):\n",
        "        \n",
        "        for i in range(0, self.size):\n",
        "            sl = self.Heap[i]\n",
        "            if self.has_rightChild(i):\n",
        "                lc = self.Heap[self.leftChild(i)]\n",
        "                rc = self.Heap[self.rightChild(i)]\n",
        "                print(\"PARENT-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] +\n",
        "                          \") ->LEFT CHILD: (\" + str(lc[0]) + \",\" + lc[1] +\n",
        "                          \") ->RIGHT CHILD: (\" + str(rc[0]) + \", \" + rc[1] + \")\")\n",
        "            elif self.has_leftChild(i):\n",
        "                lc = self.Heap[self.leftChild(i)]\n",
        "                print(\"PARENT-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] +\n",
        "                          \") ->LEFT CHILD: (\" + str(lc[0]) + \",\" + lc[1] + \")\")\n",
        "            elif self.isLeaf(i):\n",
        "                print(\"PARENT(Leaf)-\" + str(i) + \": (\" + str(sl[0]) + \",\" + sl[1] + \")\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L-75jMKrE775"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Find Success at K on the Testing Corpus\n"
      ],
      "metadata": {
        "id": "mK7Kq1wa9WIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/data/APPLING1DAT.643'\n",
        "file_ = open(path, 'r')\n",
        "lines = file_.readlines()\n",
        "\n",
        "qrel_dict = {}\n",
        "run_dict = {}\n",
        "# init qrel's and run's\n",
        "for order in ngram_order_lst:\n",
        "    qrel_dict[order] = {}\n",
        "    run_dict[order] = {}\n",
        "\n",
        "lst_k = [1, 5, 10]\n",
        "max_k = lst_k[-1]\n",
        "\n",
        "for l in lines:\n",
        "    # create correct spell and its context list\n",
        "    if '$' in l: continue\n",
        "    l_comp = l.split(\"  \")\n",
        "    word_inc = l_comp[0].lower()\n",
        "    word_cor = l_comp[1].lower()\n",
        "    token_lst = l_comp[2].split(\"\\n\")[0].lower().split(\" \")\n",
        "    end_idx = token_lst.index('*')\n",
        "    ctx_w_lst = token_lst[:end_idx]\n",
        "    \n",
        "    # predict using trained models - lm_dict\n",
        "    for order in ngram_order_lst:\n",
        "        lm = lm_dict[order]\n",
        "        cur_ctx_w_lst = []\n",
        "        if not order == 1:\n",
        "            cur_ctx_w_lst = ctx_w_lst[0 - order + 1:]\n",
        "        pred_heap = MaxHeap_Max_Score_Word(max_k)\n",
        "        pred_lst = []\n",
        "        qrel_dict[order][word_inc] = {word_cor : 1}\n",
        "        run_dict[order][word_inc] = {}\n",
        "        # find top-k largest score words => top-k smallest minus scores\n",
        "        for w in lm.vocab:\n",
        "            if not (ord('a') <= ord(w[0]) and ord(w[0]) <= ord('z')): continue\n",
        "            score = lm.logscore(w, cur_ctx_w_lst)\n",
        "            minus_score_word_pair = (0 - score, w)\n",
        "            if not pred_heap.is_full():\n",
        "                pred_heap.insert(minus_score_word_pair)\n",
        "            elif minus_score_word_pair[0] < pred_heap.topKey():\n",
        "                pred_heap.extractMax()\n",
        "                pred_heap.insert(minus_score_word_pair)\n",
        "        for i in range(1, len(lst_k) + 1):\n",
        "            cut_off = lst_k[len(lst_k) - i]\n",
        "            key_val = 1 / cut_off\n",
        "            interval = cut_off\n",
        "            if not i == len(lst_k):\n",
        "                interval -= lst_k[len(lst_k) - i - 1]\n",
        "            for j in range(interval):\n",
        "                predict = pred_heap.extractMax()[1]\n",
        "                run_dict[order][word_inc][predict] = key_val\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nRFdwsmk-JBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## Apply evaluation measures for each ngram order\n"
      ],
      "metadata": {
        "id": "K45d03g7TbAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ngram_oder in ngram_order_lst:\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(qrel_dict[ngram_oder], {'success'})\n",
        "    res = evaluator.evaluate(run_dict[ngram_oder])\n",
        "    print(ngram_oder, 'Gram Averages:')\n",
        "    for measure in list(res[list(res.keys())[0]].keys()):\n",
        "        q = [query_measures[measure] for query_measures in res.values()]\n",
        "        val = pytrec_eval.compute_aggregated_measure(measure, q)\n",
        "        print(\"  \", measure, val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5vRlxtLTm1t",
        "outputId": "fe3d05bd-59f6-4714-89f7-3566a5d36f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Gram Averages:\n",
            "   success_1 0.0\n",
            "   success_5 0.0\n",
            "   success_10 0.0\n",
            "2 Gram Averages:\n",
            "   success_1 0.005235602094240838\n",
            "   success_5 0.010471204188481676\n",
            "   success_10 0.02617801047120419\n",
            "3 Gram Averages:\n",
            "   success_1 0.015706806282722512\n",
            "   success_5 0.020942408376963352\n",
            "   success_10 0.03664921465968586\n",
            "5 Gram Averages:\n",
            "   success_1 0.015706806282722512\n",
            "   success_5 0.020942408376963352\n",
            "   success_10 0.03664921465968586\n",
            "10 Gram Averages:\n",
            "   success_1 0.015706806282722512\n",
            "   success_5 0.020942408376963352\n",
            "   success_10 0.03664921465968586\n"
          ]
        }
      ]
    }
  ]
}